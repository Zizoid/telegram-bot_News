import logging
import asyncio
import aiohttp
import feedparser
import os
import html
import hashlib
import re
import json
import time
import shutil
from datetime import datetime, timedelta
from textwrap import dedent
from bs4 import BeautifulSoup
from telegram import InputMediaPhoto
from telegram.ext import Application
from typing import Optional, Dict, List
from logging.handlers import RotatingFileHandler

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Ä–æ—Ç–∞—Ü–∏–µ–π
log_handler = RotatingFileHandler(
    'news_bot.log', 
    maxBytes=5*1024*1024,  # 5 MB
    backupCount=3
)
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    handlers=[
        log_handler,
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BOT_TOKEN = os.getenv('BOT_TOKEN') 
CHANNEL_ID = os.getenv('CHANNEL_ID') 
ADMIN_ID = os.getenv('ADMIN_ID')  # ID –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
NEWS_SOURCES = {
    "Habr": "https://habr.com/ru/rss/articles/",
    "3DNews": "https://3dnews.ru/news/rss//",
    "Ars Technica": "https://feeds.arstechnica.com/arstechnica/index/",
    "Engadget": "https://www.engadget.com/rss.xml",
    "TechCrunch": "https://techcrunch.com/feed/",
    "TechNode": "https://technode.com/feed/",
    "South China Morning Post (Tech)": "https://www.scmp.com/rss/92/feed/",
    "Nikkei Asia (Tech)": "https://www.techinasia.com/japan/feed",
    "Tech in Asia (Japan) ": "https://www.techinasia.com/japan/feed",
    "Japan Times (Tech News)": "https://www.japantimes.co.jp/search?query=feed",
}
UPDATE_INTERVAL = 300  # 5 –º–∏–Ω—É—Ç
DEEPLX_API_URL = "https://deeplx.vercel.app/translate"
MYMEMORY_API_URL = "https://api.mymemory.translated.net/get"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}
STATE_FILE = "news_bot_state.json"  # –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
BACKUP_DIR = "state_backups"  # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π —Å–æ—Å—Ç–æ—è–Ω–∏—è
MAX_HISTORY_SIZE = 1000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö—Ä–∞–Ω–∏–º—ã—Ö –∑–∞–ø–∏—Å–µ–π
MAX_CACHE_SIZE = 1000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤

# –†–µ–∑–µ—Ä–≤–Ω—ã–µ RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–∏
BACKUP_SOURCES = {
    "Reuters": "https://feeds.reuters.com/Reuters/worldNews",
    "CNN": "http://rss.cnn.com/rss/edition_world.rss",
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
RESEARCH_AGENT_ENABLED = True  # –í–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
RESEARCH_KEYWORDS = [  # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    "AI", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", 
    "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "deep learning", "LLM", "GPT",
    "–∫–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è", "quantum computing",
    "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "blockchain", "Web3",
    "–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "cybersecurity"
]
RESEARCH_MAX_LENGTH = 2000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞
RESEARCH_MIN_LENGTH = 800   # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
RESEARCH_CACHE_FILE = "research_cache.json"  # –ö—ç—à –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤

class ResearchAgent:
    """–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–≥–µ–Ω—Ç –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    def __init__(self, session: aiohttp.ClientSession):
        self.session = session
        self.cache = self.load_cache()
        self.last_cache_clear = datetime.now()
        self.api_url = "https://api.deepseek.com/v1/chat/completions"
    
    def load_cache(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π"""
        try:
            if os.path.exists(RESEARCH_CACHE_FILE):
                with open(RESEARCH_CACHE_FILE, 'r') as f:
                    return json.load(f)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: {e}")
        return {}
    
    def save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π"""
        try:
            with open(RESEARCH_CACHE_FILE, 'w') as f:
                json.dump(self.cache, f)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: {e}")
    
    def should_research(self, title: str, description: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ –Ω–æ–≤–æ—Å—Ç–∏"""
        if not RESEARCH_AGENT_ENABLED:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –æ–ø–∏—Å–∞–Ω–∏—è
        if len(description) < RESEARCH_MIN_LENGTH:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        content = f"{title} {description}".lower()
        return any(keyword.lower() in content for keyword in RESEARCH_KEYWORDS)
    
    async def research_topic(self, topic: str, max_length: int = RESEARCH_MAX_LENGTH) -> str:
        """–ü—Ä–æ–≤–æ–¥–∏—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ —Ç–µ–º–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DeepSeek API"""
        start_time = time.time()
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            cache_key = hashlib.md5(topic.encode()).hexdigest()
            if cache_key in self.cache:
                return self.cache[cache_key]
            
            # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            system_prompt = dedent("""\
                –¢—ã ‚Äî –≤–µ–¥—É—â–∏–π –∂—É—Ä–Ω–∞–ª–∏—Å—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ —Å –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã –≤ The New York Times. 
                –ü—Ä–æ–≤–µ–¥–∏ —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ —Ç–µ–º–µ, —Å–æ–±–ª—é–¥–∞—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∏–∫–∏.
                
                –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç—á–µ—Ç–∞:
                1. –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã (–æ—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã)
                2. –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ø—Ä–µ–¥—ã—Å—Ç–æ—Ä–∏—è
                3. –ê–Ω–∞–ª–∏–∑ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –∏ –¥–∞–Ω–Ω—ã—Ö
                4. –ú–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
                5. –ü—Ä–æ–≥–Ω–æ–∑—ã –∏ –±—É–¥—É—â–∏–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è
                6. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏–∑—É—á–µ–Ω–∏—è
                
                –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
                - –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã
                - –°–æ—Ö—Ä–∞–Ω—è–π –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —Ç–æ–Ω
                - –ê–¥–∞–ø—Ç–∏—Ä—É–π —Å–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –¥–ª—è —à–∏—Ä–æ–∫–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
                - –û–≥—Ä–∞–Ω–∏—á—å –æ—Ç—á–µ—Ç {} —Å–∏–º–≤–æ–ª–∞–º–∏
            """).format(max_length)
            
            # –ó–∞–ø—Ä–æ—Å –∫ DeepSeek API
            headers = {
                "Authorization": f"Bearer {os.getenv('DEEPSEEK_API_KEY')}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": topic}
                ],
                "max_tokens": max_length,
                "temperature": 0.3
            }
            
            async with self.session.post(self.api_url, json=payload, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    report = data['choices'][0]['message']['content']
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                    self.cache[cache_key] = report
                    self.save_cache()
                    
                    logger.info(f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ DeepSeek –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫")
                    return report
                else:
                    error = await response.text()
                    logger.error(f"–û—à–∏–±–∫–∞ DeepSeek API: {response.status} - {error}")
                    return f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ. –û—à–∏–±–∫–∞ API: {response.status}"
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {str(e)}")
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {str(e)}"

class NewsBot:
    def __init__(self):
        self.posted_links = set()
        self.content_checks = set()
        self.session = aiohttp.ClientSession(headers=HEADERS)
        self.app = Application.builder().token(BOT_TOKEN).build()
        self.translation_cache = {}
        self.running = False
        self.update_task = None
        self.research_agent = ResearchAgent(self.session)
        self.last_backup = datetime.now()
        self.last_cache_clear = datetime.now()
        self.load_state()

    async def send_admin_alert(self, message: str):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É"""
        if not ADMIN_ID:
            return
            
        try:
            await self.app.bot.send_message(
                chat_id=ADMIN_ID,
                text=f"üö® –û—à–∏–±–∫–∞ –±–æ—Ç–∞: {message}",
                parse_mode='Markdown'
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É: {e}")

    def load_state(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists(STATE_FILE):
                with open(STATE_FILE, 'r') as f:
                    data = json.load(f)
                    self.posted_links = set(data.get('posted_links', []))
                    self.content_checks = set(data.get('content_checks', []))
                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {len(self.posted_links)} —Å—Å—ã–ª–æ–∫, {len(self.content_checks)} —Ö–µ—à–µ–π")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
            self.send_admin_alert(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")

    def save_state(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ —Ñ–∞–π–ª"""
        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
            if len(self.posted_links) > MAX_HISTORY_SIZE:
                self.posted_links = set(list(self.posted_links)[-MAX_HISTORY_SIZE:])
            if len(self.content_checks) > MAX_HISTORY_SIZE:
                self.content_checks = set(list(self.content_checks)[-MAX_HISTORY_SIZE:])

            data = {
                'posted_links': list(self.posted_links),
                'content_checks': list(self.content_checks)
            }
            with open(STATE_FILE, 'w') as f:
                json.dump(data, f)
            logger.debug("–°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏
            if datetime.now() - self.last_backup > timedelta(days=1):
                self.create_state_backup()
                self.last_backup = datetime.now()
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
            self.send_admin_alert(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")

    def create_state_backup(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        try:
            if not os.path.exists(BACKUP_DIR):
                os.makedirs(BACKUP_DIR)
                
            backup_file = os.path.join(BACKUP_DIR, f"state_{datetime.now().strftime('%Y%m%d_%H%M')}.json")
            shutil.copy2(STATE_FILE, backup_file)
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –±—ç–∫–∞–ø—ã (>7 –¥–Ω–µ–π)
            for f in os.listdir(BACKUP_DIR):
                file_path = os.path.join(BACKUP_DIR, f)
                if os.path.isfile(file_path) and os.stat(file_path).st_mtime < (time.time() - 7 * 86400):
                    os.remove(file_path)
                    
            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è: {backup_file}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")

    def clear_caches(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞"""
        try:
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
            if len(self.translation_cache) > MAX_CACHE_SIZE:
                self.translation_cache.clear()
                logger.info("–ö—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –æ—á–∏—â–µ–Ω")
                
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Ä–∞–∑ –≤ —á–∞—Å
            if datetime.now() - self.last_cache_clear > timedelta(hours=1):
                self.research_agent.cache.clear()
                self.research_agent.save_cache()
                self.last_cache_clear = datetime.now()
                logger.info("–ö—ç—à –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –æ—á–∏—â–µ–Ω")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–µ–π: {e}")

    def escape_markdown(self, text: str) -> str:
        """–≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ MarkdownV2"""
        escape_chars = r'_*[]()~`>#+-=|{}.!'
        return re.sub(f'([{re.escape(escape_chars)}])', r'\\\1', text)

    def clean_html_content(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ HTML-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ—Ç –Ω–µ–Ω—É–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        if not text:
            return ""

        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö HTML-—Ç–µ–≥–æ–≤
        text = re.sub(r'<[^>]+>', '', text)

        # –£–¥–∞–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ-–±–ª–æ–∫–æ–≤ –∏ —Ä–µ–∫–ª–∞–º—ã
        text = re.sub(r'Video Ad Feedback', '', text, flags=re.IGNORECASE)
        text = re.sub(r'Now playing - Source:.*?$', '', text)
        text = re.sub(r'\.\.\.', '', text)
        text = re.sub(r' - CNN$', '', text)

        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
        text = re.sub(r'\d{1,2}:\d{2}', '', text)

        return text

    async def fetch_article_text(self, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ –ø–æ URL"""
        start_time = time.time()
        try:
            async with self.session.get(url, timeout=20) as response:
                if response.status == 200:
                    html_content = await response.text()

                    # –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                    cleaned_content = self.clean_html_content(html_content)

                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –±–µ–∑ HTML —Ç–µ–≥–æ–≤
                    result = ' '.join(cleaned_content.split())[:500]
                    logger.info(f"–°—Ç–∞—Ç—å—è –ø–æ–ª—É—á–µ–Ω–∞ –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫: {url}")
                    return result
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏ {url}: {e}")
        return None

    def is_russian(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç —Ä—É—Å—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã"""
        russian_letters = "–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è"
        return any(char.lower() in russian_letters for char in text)

    async def translate_with_mymemory(self, text: str, target_lang: str = "RU") -> str:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ MyMemory API"""
        if not text:
            return text

        start_time = time.time()
        try:
            params = {
                'q': text,
                'langpair': f'auto|{target_lang.lower()}'
            }
            async with self.session.get(MYMEMORY_API_URL, params=params, timeout=20) as response:
                if response.status == 200:
                    result = await response.json()
                    if result.get('responseStatus') == 200:
                        translated = result['responseData']['translatedText']
                        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞
                        if translated.isupper():
                            translated = translated.capitalize()
                        logger.info(f"MyMemory –ø–µ—Ä–µ–≤–æ–¥ –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫")
                        return translated
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —á–µ—Ä–µ–∑ MyMemory: {e}")

        return text

    async def translate_text(self, text: str, target_lang: str = "RU") -> str:
        """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–æ–≤"""
        if not text:
            return text

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Ä—É—Å—Å–∫–∏–π –ª–∏ —É–∂–µ —Ç–µ–∫—Å—Ç
        if self.is_russian(text):
            return text

        cache_key = hashlib.md5(f"{text}_{target_lang}".encode()).hexdigest()
        if cache_key in self.translation_cache:
            return self.translation_cache[cache_key]

        start_time = time.time()
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º DeepLX
        for attempt in range(2):
            try:
                payload = {
                    "text": text,
                    "source_lang": "auto",
                    "target_lang": target_lang
                }
                async with self.session.post(
                    DEEPLX_API_URL, 
                    json=payload, 
                    timeout=20
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result.get('code') == 200:
                            translated = result.get('data', text)
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
                            if self.is_russian(translated):
                                self.translation_cache[cache_key] = translated
                                logger.info(f"DeepLX –ø–µ—Ä–µ–≤–æ–¥ –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫")
                                return translated
                    await asyncio.sleep(1)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ DeepLX (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
                await asyncio.sleep(1)

        # –ï—Å–ª–∏ DeepLX –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º MyMemory
        translated = await self.translate_with_mymemory(text, target_lang)
        if self.is_russian(translated):
            self.translation_cache[cache_key] = translated
            return translated

        return text  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç

    def clean_content(self, text: str, max_length: int = 300) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return ""

        # –£–¥–∞–ª—è–µ–º –≤—Å–µ HTML-—Ç–µ–≥–∏
        text = re.sub(r'<[^>]+>', '', text)

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML-—Å—É—â–Ω–æ—Å—Ç–∏
        text = html.unescape(text)

        # –£–¥–∞–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–∫
        text = re.sub(r'Video Ad Feedback', '', text, flags=re.IGNORECASE)
        text = re.sub(r'Now playing - Source:.*?$', '', text)
        text = re.sub(r'\d{1,2}:\d{2}', '', text)
        text = re.sub(r'\.\.\.', '', text)
        text = re.sub(r' - CNN$', '', text)

        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
        text = ' '.join(text.split())

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
        return text[:max_length].strip()

    async def get_best_image(self, entry) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏"""
        start_time = time.time()
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
            sources = [
                lambda: next((media['url'] for media in entry.get('media_content', []) 
                             if media.get('type', '').startswith('image/')), None),
                lambda: next((enc['href'] for enc in entry.get('enclosures', [])
                             if enc.get('type', '').startswith('image/')), None),
                lambda: entry.get('image', {}).get('href') if isinstance(entry.get('image'), dict) else None,
                lambda: entry.get('image') if isinstance(entry.get('image'), str) else None,
                lambda: entry.get('media_thumbnail', {}).get('url')
            ]

            for source in sources:
                image_url = source()
                if image_url and image_url.startswith('http'):
                    return image_url

            # –ï—Å–ª–∏ –≤ RSS –Ω–µ—Ç, –ø–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            link = entry.get('link')
            if link:
                async with self.session.get(link, timeout=20) as response:
                    if response.status == 200:
                        html_content = await response.text()

                        # –ò—â–µ–º Open Graph –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                        og_image_match = re.search(r'<meta\s+property="og:image"\s+content="([^"]+)"', html_content)
                        if og_image_match:
                            return og_image_match.group(1)

                        # –ò—â–µ–º Twitter –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                        twitter_image_match = re.search(r'<meta\s+property="twitter:image"\s+content="([^"]+)"', html_content)
                        if twitter_image_match:
                            return twitter_image_match.group(1)

                        # –ò—â–µ–º –ø–µ—Ä–≤–æ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Å—Ç–∞—Ç—å–µ
                        img_match = re.search(r'<img[^>]+src="([^"]+)"', html_content)
                        if img_match:
                            return img_match.group(1)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        finally:
            logger.info(f"–ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞–Ω—è–ª {time.time() - start_time:.2f} —Å–µ–∫")
        return None

    async def prepare_news_message(self, entry: Dict) -> Optional[Dict]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        start_time = time.time()
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            original_title = self.clean_content(entry.get('title', ''), 120)
            if not original_title:
                logger.debug("–ü—Ä–æ–ø—É—Å–∫ –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
                return None

            link = entry.get('link', '')
            if not link:
                logger.debug("–ù–æ–≤–æ—Å—Ç—å –±–µ–∑ —Å—Å—ã–ª–∫–∏")
                return None

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ URL
            if link in self.posted_links:
                logger.debug(f"–°—Å—ã–ª–∫–∞ —É–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞—Å—å: {link}")
                return None

            # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            original_description = self.clean_content(entry.get('description', ''), 500)
            if not original_description:
                # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–µ—Ç, –ø–æ–ª—É—á–∞–µ–º –Ω–∞—á–∞–ª–æ —Å—Ç–∞—Ç—å–∏
                article_text = await self.fetch_article_text(link)
                if article_text:
                    original_description = article_text
                else:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ç–∞—Ç—å–∏: {link}")
                    return None

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–µ—à –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            content_hash = hashlib.md5(f"{original_title}{original_description}".encode()).hexdigest()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ö–µ—à—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if content_hash in self.content_checks:
                logger.debug(f"–î—É–±–ª–∏–∫–∞—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {original_title}")
                return None

            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            translated_title = await self.translate_text(original_title)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
            research_report = None
            if self.research_agent.should_research(original_title, original_description):
                logger.info(f"–ê–∫—Ç–∏–≤–∞—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è: {original_title}")
                research_topic = f"{original_title}\n\n{original_description}"
                research_report = await self.research_agent.research_topic(research_topic)
                # –ï—Å–ª–∏ –æ—Ç—á–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, —Å–æ–∫—Ä–∞—â–∞–µ–º
                if research_report and len(research_report) > RESEARCH_MAX_LENGTH:
                    research_report = research_report[:RESEARCH_MAX_LENGTH] + "..."
            
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç—á–µ—Ç
            if research_report:
                translated_description = research_report
            else:
                translated_description = await self.translate_text(original_description)

            # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_url = await self.get_best_image(entry)

            # –ü–æ–ª—É—á–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            original_category = "–û–±—â–µ–µ"
            if entry.get('tags'):
                if isinstance(entry['tags'], list) and len(entry['tags']) > 0:
                    first_tag = entry['tags'][0]
                    if isinstance(first_tag, dict) and 'term' in first_tag:
                        original_category = first_tag['term']
                    elif isinstance(first_tag, str):
                        original_category = first_tag

            translated_category = await self.translate_text(original_category)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
            source = "–ù–æ–≤–æ—Å—Ç–∏"
            for src_name, src_url in NEWS_SOURCES.items():
                if src_url in entry.get('id', '') or src_url in link:
                    source = src_name
                    break

            # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã MarkdownV2
            escaped_title = self.escape_markdown(translated_title)
            escaped_description = self.escape_markdown(translated_description)
            escaped_category = self.escape_markdown(translated_category)
            escaped_source = self.escape_markdown(source.replace(' ', ''))

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            message = (
                f"üì∞ *{escaped_title}*\n\n"
                f"{escaped_description}\n\n"
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ—Ç—á–µ—Ç
            if not research_report:
                message += (
                    f"\\#{escaped_source} \\#{escaped_category}\n"
                    f"üîó [–ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é]({link})"
                )
            else:
                message += (
                    f"üîç *–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ—Ç—á–µ—Ç* \\#{escaped_source}\n"
                    f"üîó [–ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ç–∞—Ç—å—è]({link})"
                )

            logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞–Ω—è–ª–∞ {time.time() - start_time:.2f} —Å–µ–∫")
            return {
                'message': message,
                'image_url': image_url,
                'link': link,
                'content_hash': content_hash,
                'is_research': bool(research_report)
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}", exc_info=True)
            return None

    async def send_news_item(self, news_item: Dict) -> bool:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ –≤ Telegram –∫–∞–Ω–∞–ª"""
        start_time = time.time()
        try:
            # –î–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –±–µ–∑ –ø—Ä–µ–≤—å—é
            disable_preview = news_item.get('is_research', False)
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if news_item['image_url'] and not disable_preview:
                media = InputMediaPhoto(
                    media=news_item['image_url'],
                    caption=news_item['message'],
                    parse_mode='MarkdownV2'
                )
                await self.app.bot.send_media_group(
                    chat_id=CHANNEL_ID,
                    media=[media]
                )
            else:
                await self.app.bot.send_message(
                    chat_id=CHANNEL_ID,
                    text=news_item['message'],
                    parse_mode='MarkdownV2',
                    disable_web_page_preview=disable_preview
                )

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ
            self.posted_links.add(news_item['link'])
            self.content_checks.add(news_item['content_hash'])
            self.save_state()

            logger.info(f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫: {news_item['link']}")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
            if 'content_hash' in news_item:
                self.content_checks.discard(news_item['content_hash'])
            return False

    async def fetch_feed(self, url: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ RSS-–ª–µ–Ω—Ç—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –∏ —Ä–µ–∑–µ—Ä–≤–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏"""
        start_time = time.time()
        try:
            async with self.session.get(url, timeout=25) as response:
                if response.status == 200:
                    feed_content = await response.text()
                    result = feedparser.parse(feed_content)
                    logger.info(f"RSS –ø–æ–ª—É—á–µ–Ω –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫: {url}")
                    return result
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è RSS ({url}): {e}")

            # –ü—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
            for name, source_url in NEWS_SOURCES.items():
                if source_url == url and name in BACKUP_SOURCES:
                    backup_url = BACKUP_SOURCES[name]
                    logger.info(f"–ü—Ä–æ–±—É—é —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è {name}: {backup_url}")
                    try:
                        async with self.session.get(backup_url, timeout=25) as resp:
                            if resp.status == 200:
                                feed_content = await resp.text()
                                result = feedparser.parse(feed_content)
                                logger.info(f"–†–µ–∑–µ—Ä–≤–Ω—ã–π RSS –ø–æ–ª—É—á–µ–Ω –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫: {backup_url}")
                                return result
                    except Exception as e2:
                        logger.error(f"–û—à–∏–±–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {backup_url}: {e2}")

            logger.error(f"–í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è {url} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
            return None

    async def fetch_and_publish_news(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π"""
        while self.running:
            try:
                logger.info(f"–ù–∞—á–∞–ª–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ {len(NEWS_SOURCES)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
                start_cycle_time = time.time()

                for source_name, source_url in NEWS_SOURCES.items():
                    try:
                        logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_name}")
                        feed = await self.fetch_feed(source_url)
                        if not feed or not feed.entries:
                            logger.warning(f"–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–µ: {source_name}")
                            continue

                        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(feed.entries)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {source_name}")

                        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
                        for entry in feed.entries[:3]:
                            try:
                                news_item = await self.prepare_news_message(entry)
                                if news_item:
                                    logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: {news_item.get('content_hash', '')}")
                                    success = await self.send_news_item(news_item)
                                    if success:
                                        logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {source_name}")
                                    else:
                                        logger.warning(f"–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {source_name}")

                                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–æ–π –Ω–æ–≤–æ—Å—Ç–µ–π
                                    await asyncio.sleep(25)
                                else:
                                    logger.debug("–ü—Ä–æ–ø—É—Å–∫ –Ω–æ–≤–æ—Å—Ç–∏ (–Ω–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ)")
                            except Exception as e:
                                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source_name}: {e}")

                # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π
                self.clear_caches()
                
                cycle_duration = time.time() - start_cycle_time
                logger.info(f"–¶–∏–∫–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {cycle_duration:.2f} —Å–µ–∫. –û–∂–∏–¥–∞–Ω–∏–µ {UPDATE_INTERVAL} —Å–µ–∫.")
                await asyncio.sleep(UPDATE_INTERVAL)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}", exc_info=True)
                await self.send_admin_alert(f"–û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {str(e)}")
                await asyncio.sleep(60)

    async def run(self):
        """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
        self.running = True
        await self.app.initialize()
        await self.app.start()
        logger.info(f"–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –ö–∞–Ω–∞–ª: {CHANNEL_ID}")
        self.update_task = asyncio.create_task(self.fetch_and_publish_news())
        logger.info("–§–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞–ø—É—â–µ–Ω")

    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–æ—Ç–∞"""
        logger.info("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–æ—Ç–∞...")
        self.running = False

        if self.update_task:
            self.update_task.cancel()
            try:
                await self.update_task
            except asyncio.CancelledError:
                logger.info("–§–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

        self.save_state()
        await self.app.stop()
        await self.app.shutdown()
        await self.session.close()
        logger.info("–ë–æ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

async def main():
    bot = NewsBot()
    try:
        await bot.run()
        while bot.running:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        logger.info("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ Ctrl+C...")
    except Exception as e:
        logger.error(f"–§–∞—Ç–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        await bot.send_admin_alert(f"–§–∞—Ç–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
    finally:
        await bot.stop()

if __name__ == "__main__":
    asyncio.run(main())
